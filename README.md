
# Data Modeling with Postgres for Sparkify

## Goal

Structure the data that Sparkify (fictional company) has been collecting on songs and user activity for their new music streaming app for analysis.

This project is part of Udacity's Data Engineering Nanodegree

## Data

### Song Dataset

- This dataset is a subset of real song data from the Million Song Dataset.
- Each file is in JSON format and contains metadata about a song and the artist of that song.
  - The files are partitioned by the first three letters of each song's track ID. For example:
    - song_data/A/B/C/TRABCEI128F424C983.json
    - song_data/A/A/B/TRAABJL12903CDCF1A.json

### Log Dataset

- This dataset is event data generated by an event simulator based on the songs in the dataset above.
- Each file is in JSON format and contains data about user events in the app.
  - The files are partitioned by year and month. For example:
    - log_data/2018/11/2018-11-12-events.json
    - log_data/2018/11/2018-11-13-events.json


## Data Models

### Entities

- songplays: records in log data associated with song plays
- users: users in the app
- songs: songs in music database
- artists: artists in music database
- time: timestamps of records in songplays broken down into specific units

### Entity Relationship Diagram (ERD)

![Alt text](sparkify_ERD.png?raw=true "Sparkify ERD")


## Installation

Clone the repo onto your machine with the following command:

$ git checkout sparkify_postgres


## Dependencies

I use Python 3.6.

See https://www.python.org/downloads/ for information on download.

I use virtualenv to manage dependencies, if you have it installed you can run
the following commands from the root code directory to create the environment and
activate it:

$ python3 -m venv venv
$ source venv/bin/activate

See https://virtualenv.pypa.io/en/stable/ for more information.

----

I use pip to install dependencies, which comes installed in a virtualenv.
You can run the following to install dependencies:

$ pip install -r requirements.txt

See https://pip.pypa.io/en/stable/installing/ for more information.

----

I use a locally hosted PostgreSQL database.

See https://www.postgresql.org/ for more information.


## Usage

There are several main scripts that can be executed:

- src/create_tables.py: Drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
  - src/test.ipynb displays the first few rows of each table to let you check your database.
- src/etl.py: Reads and processes files from song_data and log_data and loads them into your tables.
  - src/etl.ipynb reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
- src/sql_queries.py: Contains all your sql queries, and is used during ETL process.


## Future Optimizations

- Create config.py to specify environment variables like DB user, etc
- Create foreign key relations for each table
- Create a separate table for Sessions
- Insert data using the COPY command to bulk insert log files instead of using INSERT on one row at a time
- Add data quality checks
- Create a dashboard for analytic queries on your new database
